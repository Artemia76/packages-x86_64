diff -Naur a/src/vboxhost/vboxdrv/linux/SUPDrv-linux.c b/src/vboxhost/vboxdrv/linux/SUPDrv-linux.c
--- a/src/vboxhost/vboxdrv/linux/SUPDrv-linux.c	2019-03-15 17:12:45.000000000 +0100
+++ b/src/vboxhost/vboxdrv/linux/SUPDrv-linux.c	2020-08-11 23:09:41.986652581 +0200
@@ -756,12 +756,19 @@
 RTCCUINTREG VBOXCALL supdrvOSChangeCR4(RTCCUINTREG fOrMask, RTCCUINTREG fAndMask)
 {
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 20, 0)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
     RTCCUINTREG uOld = this_cpu_read(cpu_tlbstate.cr4);
+#else
+    RTCCUINTREG uOld = __read_cr4();
+#endif
     RTCCUINTREG uNew = (uOld & fAndMask) | fOrMask;
     if (uNew != uOld)
     {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
         this_cpu_write(cpu_tlbstate.cr4, uNew);
         __write_cr4(uNew);
+#endif
+        ASMSetCR4(uNew);
     }
 #else
     RTCCUINTREG uOld = ASMGetCR4();
diff -Naur a/src/vboxhost/vboxdrv/r0drv/linux/alloc-r0drv-linux.c b/src/vboxhost/vboxdrv/r0drv/linux/alloc-r0drv-linux.c
--- a/src/vboxhost/vboxdrv/r0drv/linux/alloc-r0drv-linux.c	2019-10-30 15:16:23.000000000 +0100
+++ b/src/vboxhost/vboxdrv/r0drv/linux/alloc-r0drv-linux.c	2020-08-11 23:45:54.466642385 +0200
@@ -151,6 +151,8 @@
 
 
 #ifdef RTMEMALLOC_EXEC_VM_AREA
+
+
 /**
  * Allocate executable kernel memory in the module range.
  *
@@ -166,7 +168,12 @@
     struct vm_struct   *pVmArea;
     size_t              iPage;
 
+# if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 8, 0)
+    pVmArea = __get_vm_area_caller(cbAlloc, VM_ALLOC, MODULES_VADDR, MODULES_END,
+                                   __builtin_return_address(0));
+#else
     pVmArea = __get_vm_area(cbAlloc, VM_ALLOC, MODULES_VADDR, MODULES_END);
+#endif
     if (!pVmArea)
         return NULL;
     pVmArea->nr_pages = 0;    /* paranoia? */
@@ -199,6 +206,12 @@
 # endif
         pVmArea->nr_pages = cPages;
         pVmArea->pages    = papPages;
+# if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 8, 0)
+        unsigned long start = (unsigned long)pVmArea->addr;
+        unsigned long size = get_vm_area_size(pVmArea);
+
+        if (!map_kernel_range(start, size, PAGE_KERNEL_EXEC, papPages))
+#else
         if (!map_vm_area(pVmArea, PAGE_KERNEL_EXEC,
 # if LINUX_VERSION_CODE < KERNEL_VERSION(3, 17, 0)
                          &papPagesIterator
@@ -206,6 +219,7 @@
                          papPages
 # endif
                          ))
+#endif
         {
             PRTMEMLNXHDREX pHdrEx = (PRTMEMLNXHDREX)pVmArea->addr;
             pHdrEx->pVmArea     = pVmArea;
diff -Naur a/src/vboxhost/vboxdrv/r0drv/linux/memobj-r0drv-linux.c b/src/vboxhost/vboxdrv/r0drv/linux/memobj-r0drv-linux.c
--- a/src/vboxhost/vboxdrv/r0drv/linux/memobj-r0drv-linux.c	2020-04-02 08:11:13.000000000 +0200
+++ b/src/vboxhost/vboxdrv/r0drv/linux/memobj-r0drv-linux.c	2020-08-11 23:02:38.579987899 +0200
@@ -220,9 +220,17 @@
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
         ulAddr = vm_mmap(NULL, R3PtrFixed, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS | MAP_FIXED, 0);
 #else
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
         down_write(&pTask->mm->mmap_sem);
+#else
+        down_write(&pTask->mm->mmap_lock);
+#endif
         ulAddr = do_mmap(NULL, R3PtrFixed, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS | MAP_FIXED, 0);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
         up_write(&pTask->mm->mmap_sem);
+#else
+        up_write(&pTask->mm->mmap_lock);
+#endif
 #endif
     }
     else
@@ -230,9 +238,17 @@
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
         ulAddr = vm_mmap(NULL, 0, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS, 0);
 #else
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
         down_write(&pTask->mm->mmap_sem);
+#else
+        down_write(&pTask->mm->mmap_lock);
+#endif
         ulAddr = do_mmap(NULL, 0, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS, 0);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
         up_write(&pTask->mm->mmap_sem);
+#else
+        up_write(&pTask->mm->mmap_lock);
+#endif
 #endif
         if (    !(ulAddr & ~PAGE_MASK)
             &&  (ulAddr & (uAlignment - 1)))
@@ -267,13 +283,29 @@
     Assert(pTask == current); RT_NOREF_PV(pTask);
     vm_munmap((unsigned long)pv, cb);
 #elif defined(USE_RHEL4_MUNMAP)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
     down_write(&pTask->mm->mmap_sem);
+#else
+    down_write(&pTask->mm->mmap_lock);
+#endif
     do_munmap(pTask->mm, (unsigned long)pv, cb, 0); /* should it be 1 or 0? */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
     up_write(&pTask->mm->mmap_sem);
 #else
+    up_write(&pTask->mm->mmap_lock);
+#endif
+#else
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
     down_write(&pTask->mm->mmap_sem);
+#else
+    down_write(&pTask->mm->mmap_lock);
+#endif
     do_munmap(pTask->mm, (unsigned long)pv, cb);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
     up_write(&pTask->mm->mmap_sem);
+#else
+    up_write(&pTask->mm->mmap_lock);
+#endif
 #endif
 }
 
@@ -591,7 +623,11 @@
                 size_t              iPage;
                 Assert(pTask);
                 if (pTask && pTask->mm)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
                     down_read(&pTask->mm->mmap_sem);
+#else
+                    down_read(&pTask->mm->mmap_lock);
+#endif
 
                 iPage = pMemLnx->cPages;
                 while (iPage-- > 0)
@@ -606,7 +642,11 @@
                 }
 
                 if (pTask && pTask->mm)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
                     up_read(&pTask->mm->mmap_sem);
+#else
+                    up_read(&pTask->mm->mmap_lock);
+#endif
             }
             /* else: kernel memory - nothing to do here. */
             break;
@@ -1073,7 +1113,11 @@
     papVMAs = (struct vm_area_struct **)RTMemAlloc(sizeof(*papVMAs) * cPages);
     if (papVMAs)
     {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
         down_read(&pTask->mm->mmap_sem);
+#else
+        down_read(&pTask->mm->mmap_lock);
+#endif
 
         /*
          * Get user pages.
@@ -1158,7 +1202,11 @@
                 papVMAs[rc]->vm_flags |= (VM_DONTCOPY | VM_LOCKED);
             }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
             up_read(&pTask->mm->mmap_sem);
+#else
+            up_read(&pTask->mm->mmap_lock);
+#endif
 
             RTMemFree(papVMAs);
 
@@ -1185,7 +1233,11 @@
 #endif
         }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
         up_read(&pTask->mm->mmap_sem);
+#else
+        up_read(&pTask->mm->mmap_lock);
+#endif
 
         RTMemFree(papVMAs);
         rc = VERR_LOCK_FAILED;
@@ -1595,7 +1647,11 @@
             const size_t    cPages = pMemLnxToMap->Core.cb >> PAGE_SHIFT;
             size_t          iPage;
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
             down_write(&pTask->mm->mmap_sem);
+#else
+            down_write(&pTask->mm->mmap_lock);
+#endif
 
             rc = VINF_SUCCESS;
             if (pMemLnxToMap->cPages)
@@ -1712,7 +1768,11 @@
             }
 #endif /* CONFIG_NUMA_BALANCING */
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
             up_write(&pTask->mm->mmap_sem);
+#else
+            up_write(&pTask->mm->mmap_lock);
+#endif
 
             if (RT_SUCCESS(rc))
             {
